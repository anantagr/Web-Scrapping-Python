{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importing required libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# %matplotlib inline\n",
    "# import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "# import matplotlib.dates as mdates\n",
    "# import matplotlib.ticker as ticker\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Declaring the url for scrapping__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = 'https://www.liberal.ca/blog'\n",
    "uClient = uReq(my_url) #Grabbing the webpage stored in mu_url\n",
    "page_html = uClient.read() \n",
    "uClient.close() #Close the web client after grabbing the data\n",
    "page_soup = soup(page_html, \"html.parser\") #Parsing the file in html format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Finding number of pages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 457,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "75\n"
    }
   ],
   "source": [
    "pages = [i.text for i in page_soup.find_all('a',{'class':'page'}) if 'https://www.liberal.ca/blog/page' in str(i)]\n",
    "lastpage = pages[-1]\n",
    "print(lastpage)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For multiple pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to run all the pages and store each blog in blog_container\n",
    "\n",
    "# for cp in np.arange(1,int(lastpage)+1):\n",
    "#     my_url = 'https://www.liberal.ca/blog/page/' + str(cp) + \"/\"\n",
    "#     uClient = uReq(my_url)                              #Grabbing the webpage stored in mu_url\n",
    "#     page_html = uClient.read()\n",
    "#     page_soup = soup(page_html, \"html.parser\")          #Parsing the file in html format\n",
    "#     blog_containers = page_soup.findAll('div',{'class':'column large-4 medium-6 small-12'})   #grabbing each blog\n",
    "#     time.sleep(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For a single page, 9 blogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Running loop for all blogs on page 1__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [],
   "source": [
    "blog_containers = page_soup.findAll('div',{'class':'column large-4 medium-6 small-12'})   #grabbing each blog container\n",
    "blog_count = 0\n",
    "filename = 'liberal_blog.txt'\n",
    "f = open(filename, 'w', encoding='utf-8')\n",
    "\n",
    "# headers = \"Title\", \"Date\", \"Link\", \"Content/n\"\n",
    "\n",
    "for blog in blog_containers:\n",
    "    blog_url = blog.a[\"href\"]\n",
    "    uClient = uReq(blog_url) #Grabbing the webpage stored in mu_url\n",
    "    page_html = uClient.read() \n",
    "    page_soup = soup(page_html, \"html.parser\") #Parsing the file in html format\n",
    "    title = page_soup.h1.text\n",
    "    date = page_soup.find('p', attrs = {'class':'byline'}).text\n",
    "    blog_content = page_soup.findAll('div', attrs = {'class':'blog-content'})\n",
    "    blog_count +=1\n",
    "    f.write(\"Blog No.:\"      + str(blog_count)   + \"\\n\" + \n",
    "            \"Blog Title: \"   + title             + \"\\n\" + \n",
    "            \"Publish Date: \" + date              + \"\\n\" + \n",
    "            \"Blog link: \"    + blog_url          + \"\\n\" + \n",
    "            \"Content\"        + str(blog_content) + \"\\n\" + \"\\n\")\n",
    "    # f.write(\"Blog No.:\" + str(blog_count) + \"/n\" + \"Blog Title: \" + title + \"/n\" + \"Publish Date: \" + date + \"/n\" + \"Blog link: \" + blog_url + \"/n\" + \"Content\" + str(blog_content) + \"/n/n\")\n",
    "f.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}