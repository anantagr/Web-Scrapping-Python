{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web scraping application for University of Calgary - news page\n",
    "\n",
    "## Author:   Anant Agarwal\n",
    "## Date:     June 16, 2020\n",
    "\n",
    "### Description: \n",
    "#### The intent of this script is to scrape data from blogs posted on University of Calgary news page and store them in a csv file on local disk. The user of this application will have to have choices on how the data should be scraped from the website\n",
    "    1) Collect data from a fixed number of articles.\n",
    "    2) Collect data from all the articles written after a fixed date. \n",
    "\n",
    "### Information provided by the user:  \n",
    "    1) Number of articles to be scraped, or\n",
    "    2) Date, scrape all the articles after that date, or\n",
    "    3) Both\n",
    "\n",
    "### Using the Application:\n",
    "-   For running the script, User have to call the function getArticle and pass the date and number of blogs       to be scraped.\n",
    "-   E.g. __getArticle('Jun 03, 2020', '20')__\n",
    "-   If user does not have any values to pass, we can leave the entry blank. E.g. __getArticle('', '20').          Script will consider empty value as 'null'\n",
    "-   Please enter the date in __mmm dd, yyyy__ format __(e.g. June 10, 2019)__.\n",
    "-   If both values are passed as 'null' then the program will terminate with a message that no values were        entered.\n",
    "\n",
    "### Result:\n",
    "#### All the results will be stored in a uCal.csv file in the same directory where application is stored\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importing required libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  pandas as pd\n",
    "import  numpy  as np\n",
    "import  datetime\n",
    "from    urllib.request import urlopen as uReq\n",
    "from    bs4 import BeautifulSoup as soup   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date function\n",
    "\n",
    "In this function, we are asking user for date filter. \n",
    "* If user enters no value, 'null' value will be returned and function will print 'No date entered'\n",
    "* If user give data in incorrect format, error message will be prompted and user will be requested to re-enter\n",
    "* If user give data in correct format, date will be returned and function will print 'Date entered successfully' \n",
    "\n",
    "\n",
    "__Things to do__: Put a check on future dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getDate(user_date):\n",
    "    global user_date_convert\n",
    "    \"\"\"\n",
    "    Args: None\n",
    "    Returns: Date value in datetime format or 'null'\n",
    "    \"\"\"\n",
    "    # user_date = input(\"Enter the date (mmm dd, yyyy): \") #promting user to enter a date in given format\n",
    "\n",
    "    #Checking if any value has been entered using 'len' command\n",
    "    if (len(user_date) != 0):\n",
    "        try:\n",
    "            #Converting user value in datetime format, if successful value will be stored in user_date_convert\n",
    "            user_date_convert = datetime.datetime.strptime(user_date, '%b %d, %Y')\n",
    "            print(\"Date entered successfully\")\n",
    "            \n",
    "        except ValueError:\n",
    "            #If format of value entered is incorrect them print error message\n",
    "            sys.exit(\"***ERROR: Incorrect Format, please enter date in mmm dd, yyyy format\")\n",
    "\n",
    "    else:\n",
    "        #If no value entered, return 'null'\n",
    "        user_date_convert = 'null'\n",
    "        print (\"No Date entered\")\n",
    "    \n",
    "    return (user_date_convert)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Number of blog function\n",
    "\n",
    "In this function, we are asking user to provide number of blogs to be scraped\n",
    "* If user enters no value, 'null' value will be returned and function will print 'No data entered'\n",
    "* If user give data in incorrect format i.e. non-integral or non-positive value, error message will be prompted and user will be requested to re-enter\n",
    "* If user give data in correct format, number of blogs will be returned and function will print 'Number of blogs value entered successfully' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBlog_count(blog_count):\n",
    "    global user_blog_count\n",
    "    \"\"\"\n",
    "    Args: None\n",
    "    Returns: Number of blogs value or 'null'\n",
    "    \"\"\"    \n",
    "    #Checking if any value has been entered using 'len' command\n",
    "    if (len(blog_count) != 0):\n",
    "        try:\n",
    "            #Converting user value in integar format, if successful value will be stored in user_blog_count\n",
    "            user_blog_count = int(blog_count)\n",
    "\n",
    "            #Checking if the integar value us non-positive, if yes, error message will prompt user to re-enter\n",
    "            if(user_blog_count < 1):\n",
    "                sys.exit(\"***ERROR: Cannot accept value less than 1, please re-enter\")\n",
    "            else:\n",
    "                #Value is positive integar\n",
    "                print(\"Number of blogs value entered successfully\")\n",
    "                # return (user_blog_count)\n",
    "            \n",
    "        except ValueError:\n",
    "            #If the value is non-integral, prompt the error message\n",
    "            sys.exit(\"***ERROR: Non-integral value entered, please re-enter\")\n",
    "\n",
    "    else:\n",
    "        #If no value entered, return 'null'\n",
    "        user_blog_count = 'null'\n",
    "        print (\"No Blog count entered\")\n",
    "    \n",
    "    return (user_blog_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total number of pages in the website.\n",
    "\n",
    "* In this, we have defined a function __getPage_count__, which will take URL of the website as an input and count the total number of pages.\n",
    "\n",
    "* We look through the HTML code of the first page and find a __list__ with __class name: \"pager_item pager_item--last\"__\n",
    "\n",
    "* Extracted the final page value from there and stored it in integar format.\n",
    "\n",
    "* Returned the final page value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage_count(my_url: str):\n",
    "    \"\"\"\n",
    "    Args: my_url -> link of the University of Calgary - News website\n",
    "    Returns: pages -> total number of pages in integar format\n",
    "    \"\"\"\n",
    "\n",
    "    # Regular 'bs4' format.\n",
    "    uClient = uReq(my_url) #Grabbing the webpage stored in my_url\n",
    "    page_html = uClient.read() \n",
    "    uClient.close() #Close the web client after grabbing the data\n",
    "    page_soup = soup(page_html, \"html.parser\") #Parsing the file in html format\n",
    "\n",
    "\n",
    "    # 'pages' will store the final page value and returned it back.\n",
    "    pages = int(page_soup.find('li', {'class':'pager__item pager__item--last'}).a[\"href\"].replace(\"?search_api_fulltext=&page=\",\"\"))\n",
    "    print(str(pages) + \" pages counted successfully\")\n",
    "    return (pages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function: Blog containers in individual page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getBlog_containers(last_page, user_date, user_blog_count):\n",
    "    \"\"\"\n",
    "    Args: URL of University of Calgary - News website\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    blog_count = 0\n",
    "    my_url = 'https://www.ucalgary.ca/news/all-ucalgary-news' #University of Calgary - News website\n",
    "\n",
    "    total_pages = range(last_page) # setting up range function from page 0 to last page\n",
    "    \n",
    "    #Taking one page at a time and defining all the blogs in it\n",
    "    for current_page in total_pages:\n",
    "        page_url = my_url + '?search_api_fulltext=&page=' + str(current_page)\n",
    "        uClient = uReq(page_url) #Grabbing the webpage stored in mu_url\n",
    "        page_html = uClient.read() \n",
    "        uClient.close() #Close the web client after grabbing the data\n",
    "        page_soup = soup(page_html, \"html.parser\") #Parsing the file in html format\n",
    "\n",
    "        #find the main container that stores all the blogs\n",
    "        # blog_containers = page_soup.findAll('div',{'class':['news-teaser news-article-teaser', 'col-sm-12 col-md-6 news-items__main', 'col-sm-12 col-lg-4 news-item']})\n",
    "        blog_containers = page_soup.findAll('div',{'class':['news-teaser news-article-teaser']})\n",
    "\n",
    "        #Find the website link for individual blog page from each blog in the container\n",
    "        for blog in blog_containers:\n",
    "\n",
    "            blog_url = 'https://www.ucalgary.ca/' + blog.a[\"href\"]\n",
    "            blogClient = uReq(blog_url) #Grabbing the webpage stored in mu_url\n",
    "            blog_html = blogClient.read() \n",
    "            blogClient.close() #Close the web client after grabbing the data\n",
    "            blog_soup = soup(blog_html, \"html.parser\") #Parsing the file in html format\n",
    "            blog_date = blog_soup.find('p', attrs = {'class':'title'}).text\n",
    "            blog_date_convert = datetime.datetime.strptime(blog_date, '%B %d, %Y') # Converted date from string to datetime format\n",
    "            dublicate_url_check = checkDublicate(blog_url)\n",
    "            \n",
    "            if (dublicate_url_check == False):                              \n",
    "                \n",
    "                blog_title = blog_soup.find('h1', attrs = {'class':'head'}).text\n",
    "                blog_subtitle = blog_soup.find('h4', attrs = {'class':'deck'}).text\n",
    "                blog_body = blog_soup.findAll('div', {'class':'body'})[0].text.replace(\"\\n\",\"\").replace(\"\\xad—\\xa0\",\"\").replace(\"—\\xa0\",\"\").replace(\"\\xa0\",\"\").replace(\"\\xad\",\"\")            \n",
    "                \n",
    "                if(user_date != 'null'):\n",
    "                    if(blog_date_convert < user_date_convert):\n",
    "                        print(\"User provided date reached\")\n",
    "                        return\n",
    "\n",
    "                if(user_blog_count != 'null'):\n",
    "                    if(blog_count >= user_blog_count):\n",
    "                        print(\"Number of blogs reached\")\n",
    "                        return\n",
    "                \n",
    "                writeData(blog_count+1, blog_title, blog_subtitle, blog_date, blog_url, blog_body)\n",
    "            \n",
    "            else:\n",
    "                print(\"Blog \" + str(blog_count+1) + \" already written\")\n",
    "                \n",
    "                if(user_date != 'null'):\n",
    "                    if(blog_date_convert < user_date_convert):\n",
    "                        print(\"User provided date reached\")\n",
    "                        return\n",
    "\n",
    "                if(user_blog_count != 'null'):\n",
    "                    if(blog_count+1 >= user_blog_count):\n",
    "                        print(\"Number of blogs reached\")\n",
    "                        return\n",
    "\n",
    "            #Increment blog counter after scraping 1 blog\n",
    "            blog_count += 1\n",
    "        \n",
    "\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the dublicate entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the entry\n",
    "def checkDublicate(blog_url):\n",
    "    \"\"\"\n",
    "    Args: URL of University of Calgary - News website\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    colnames = ['blog_count', 'blog_title', 'blog_subtitle', 'blog_date', 'blog_url', 'blog_body']\n",
    "    data = pd.read_csv('UCal_csv.csv', names=colnames,  engine = 'python')[1:]\n",
    "    url_list = data.blog_url.tolist()\n",
    "    if (blog_url in url_list):\n",
    "        return (True)\n",
    "    else:\n",
    "        return(False)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structuring the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structureCSV():\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # colnames = ['blog_count', 'blog_title', 'blog_subtitle', 'blog_date', 'blog_url', 'blog_body']\n",
    "        # pd.read_csv('UCal_csv.csv','rU'), encoding='utf-8', engine='c', names=colnames)[1:]\n",
    "        pd.read_csv('UCal_csv.csv', engine = 'python')[1:]\n",
    "        print(\"Old csv file opened successfully\")\n",
    "        return\n",
    "    except FileNotFoundError:\n",
    "\n",
    "        filename = 'UCal_csv.csv' #File name to store the scraped data\n",
    "        f = open(filename, 'w', encoding='utf-8')\n",
    "\n",
    "        #Defining header for csv\n",
    "        headers = 'Blog No., Blog Title, Blog Subtitle, Blog Publish Data, Blog Link, Blog Content\\n'\n",
    "        f.write(headers)\n",
    "        f.close()\n",
    "        print(\"New csv file opened successfully\")\n",
    "\n",
    "\n",
    "    filename = 'UCal_txt.txt' #File name to store the scraped data\n",
    "    a = open(filename, 'a', encoding='utf-8')\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeData(blog_count, blog_title, blog_subtitle, blog_date, blog_url, blog_body):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    filename = 'UCal_csv.csv' #File name to store the scraped data\n",
    "    f = open(filename, 'a', encoding='utf-8')\n",
    "\n",
    "    f.write(str(blog_count).replace(\",\", \"\")     + \",\" + \n",
    "                blog_title.replace(\",\", \"\")      + \",\" +\n",
    "                blog_subtitle.replace(\",\", \"\")   + \",\" +\n",
    "                blog_date.replace(\",\", \"\")       + \",\" +\n",
    "                blog_url.replace(\",\", \"\")        + \",\" +\n",
    "                blog_body.replace(\",\", \"\")       + \"\\n\")\n",
    "\n",
    "    f.close()\n",
    "    \n",
    "    filename = 'UCal_txt.txt' #File name to store the scraped data\n",
    "    a = open(filename, 'a', encoding='utf-8')\n",
    "\n",
    "    a.write(\"Blog No.:\"         + str(blog_count)   + \"\\n\" + \n",
    "            \"Blog Title: \"      + blog_title        + \"\\n\" + \n",
    "            \"Blog Subtitle: \"   + blog_subtitle     + \"\\n\" +\n",
    "            \"Publish Date: \"    + blog_date         + \"\\n\" + \n",
    "            \"Blog link: \"       + blog_url          + \"\\n\" + \n",
    "            \"Content: \"         + str(blog_body)    + \"\\n\" + \"\\n\")\n",
    "    a.close()\n",
    "\n",
    "    print(\"Blog \" + str(blog_count) + \" written successfully\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine Function\n",
    "\n",
    "### In this function, application will ask the user to enter the choice for scraping the blogs from the website\n",
    "1. If the user want to scrape data from a set number of blogs \n",
    "2. If the user wants to scrape data from a given date\n",
    "\n",
    "#### Based on the input from the user, appropriate function will be called:\n",
    "- getData_number\n",
    "- getData_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getArticle(date, blog_count):\n",
    "    \n",
    "    my_url = 'https://www.ucalgary.ca/news/all-ucalgary-news' #University of Calgary - News website\n",
    "    user_date = getDate(date)\n",
    "    user_blog_count = getBlog_count(blog_count)\n",
    "    if(user_date == 'null' and user_blog_count == 'null'):\n",
    "        print (\"Program terminated as no Date or Blog count provided\")\n",
    "    else:\n",
    "        last_page = getPage_count(my_url)\n",
    "        structureCSV()\n",
    "        container_data = getBlog_containers(last_page, user_date, user_blog_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "getArticle('', '10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}