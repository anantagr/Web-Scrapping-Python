{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importing required libraries__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as soup   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Declaring the url for scrapping__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_url = 'https://www.ucalgary.ca/news/all-ucalgary-news'   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Function to find number of pages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPage_count(my_url: str):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "    uClient = uReq(my_url) #Grabbing the webpage stored in mu_url\n",
    "    page_html = uClient.read() \n",
    "    uClient.close() #Close the web client after grabbing the data\n",
    "    page_soup = soup(page_html, \"html.parser\") #Parsing the file in html format\n",
    "\n",
    "    pages = int(page_soup.find('li', {'class':'pager__item pager__item--last'}).a[\"href\"].replace(\"?search_api_fulltext=&page=\",\"\"))\n",
    "    return (pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = getPage_count(my_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Function to scrape based on number of blogs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData_number(user_blog_count):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    user_blog_count = int(input(\"Enter number of blog you want to scrape: \"))\n",
    "\n",
    "    blog_count = 0\n",
    "\n",
    "    total_pages = range(2) # setting up range function from page 0 to last page\n",
    "\n",
    "    for current_page in total_pages:\n",
    "        page_url = my_url + '?search_api_fulltext=&page=' + str(current_page)\n",
    "        uClient = uReq(page_url) #Grabbing the webpage stored in mu_url\n",
    "        page_html = uClient.read() \n",
    "        uClient.close() #Close the web client after grabbing the data\n",
    "        page_soup = soup(page_html, \"html.parser\") #Parsing the file in html format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #find the main container that stores all the blogs\n",
    "        blog_containers = page_soup.findAll('div',{'class':'news-teaser news-article-teaser'})\n",
    "\n",
    "\n",
    "        #Setting up a counter to count number of blogs scraped\n",
    "\n",
    "\n",
    "        #Find the website link for individual blog page from each blog in the container\n",
    "        for blog in blog_containers:\n",
    "            blog_url = 'https://www.ucalgary.ca/' + blog.a[\"href\"]\n",
    "            blogClient = uReq(blog_url) #Grabbing the webpage stored in mu_url\n",
    "            blog_html = blogClient.read() \n",
    "            blogClient.close() #Close the web client after grabbing the data\n",
    "            blog_soup = soup(blog_html, \"html.parser\") #Parsing the file in html format\n",
    "            blog_title = blog_soup.find('h1', attrs = {'class':'head'}).text\n",
    "            blog_subtitle = blog_soup.find('h4', attrs = {'class':'deck'}).text\n",
    "            blog_date = blog_soup.find('p', attrs = {'class':'title'}).text\n",
    "            blog_date_convert = datetime.datetime.strptime(date, '%B %d, %Y') # Converted date from string to datetime format\n",
    "            blog_body = blog_soup.findAll('div', {'class':'body'})[0].text.replace(\"\\n\",\"\").replace(\"\\xad窶能\xa0\",\"\").replace(\"窶能\xa0\",\"\").replace(\"\\xa0\",\"\").replace(\"\\xad\",\"\")\n",
    "\n",
    "            blog_count += 1\n",
    "\n",
    "            # if (blog_date_convert < user_date_convert):\n",
    "            if (blog_count > user_blog_count):\n",
    "                break\n",
    "        \n",
    "        # if (blog_date_convert < user_date_convert):\n",
    "        if (blog_count > user_blog_count):\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "20"
     },
     "metadata": {},
     "execution_count": 199
    }
   ],
   "source": [
    "blog_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Function to scrape based on date of blogs__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData_date(user_date_convert):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    # user_date = (input(\"Enter the date (mmm dd, yyyy): \"))\n",
    "    # try:\n",
    "    #     user_date_convert = datetime.datetime.strptime(user_date, '%b %d, %Y')\n",
    "    # except ValueError:\n",
    "    #     print(\"Incorrect Format\")\n",
    "\n",
    "    blog_count = 0\n",
    "\n",
    "    total_pages = range(2) # setting up range function from page 0 to last page\n",
    "\n",
    "    for current_page in total_pages:\n",
    "        page_url = my_url + '?search_api_fulltext=&page=' + str(current_page)\n",
    "        uClient = uReq(page_url) #Grabbing the webpage stored in mu_url\n",
    "        page_html = uClient.read() \n",
    "        uClient.close() #Close the web client after grabbing the data\n",
    "        page_soup = soup(page_html, \"html.parser\") #Parsing the file in html format\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #find the main container that stores all the blogs\n",
    "        blog_containers = page_soup.findAll('div',{'class':'news-teaser news-article-teaser'})\n",
    "\n",
    "\n",
    "        #Setting up a counter to count number of blogs scraped\n",
    "\n",
    "\n",
    "        #Find the website link for individual blog page from each blog in the container\n",
    "        for blog in blog_containers:\n",
    "            blog_url = 'https://www.ucalgary.ca/' + blog.a[\"href\"]\n",
    "            blogClient = uReq(blog_url) #Grabbing the webpage stored in mu_url\n",
    "            blog_html = blogClient.read() \n",
    "            blogClient.close() #Close the web client after grabbing the data\n",
    "            blog_soup = soup(blog_html, \"html.parser\") #Parsing the file in html format\n",
    "            blog_title = blog_soup.find('h1', attrs = {'class':'head'}).text\n",
    "            blog_subtitle = blog_soup.find('h4', attrs = {'class':'deck'}).text\n",
    "            blog_date = blog_soup.find('p', attrs = {'class':'title'}).text\n",
    "            blog_date_convert = datetime.datetime.strptime(date, '%B %d, %Y') # Converted date from string to datetime format\n",
    "            blog_body = blog_soup.findAll('div', {'class':'body'})[0].text.replace(\"\\n\",\"\").replace(\"\\xad窶能\xa0\",\"\").replace(\"窶能\xa0\",\"\").replace(\"\\xa0\",\"\").replace(\"\\xad\",\"\")\n",
    "\n",
    "            blog_count += 1\n",
    "\n",
    "            if (blog_date_convert < user_date_convert):\n",
    "                break\n",
    "        \n",
    "        if (blog_date_convert < user_date_convert):\n",
    "            break\n",
    "            # print(blog_url)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main function\n",
    "\n",
    "### In this function, application will ask the user to enter the choice for scraping the blogs from the website\n",
    "1. If the user want to scrape data from a set number of blogs \n",
    "2. If the user wants to scrape data from a given date\n",
    "\n",
    "#### Based on the input from the user, appropriate function will be called:\n",
    "- getData_number\n",
    "- getData_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = int(input(\"\"\"Please enter your choice to scrape data:  \n",
    "                    (1) - To scrape data based on number of blogs. \n",
    "                    (2) - To scrape data from a given data. \"\"\"))\n",
    "\n",
    "if(user_input == 1):\n",
    "    user_blog_count = input(\"Enter number of blog you want to scrape: \")\n",
    "    blog_number = getData_number(int(user_blog_count))\n",
    "    \n",
    "elif(user_input == 2):\n",
    "    user_date = (input(\"Enter the date (mmm dd, yyyy): \"))\n",
    "    try:\n",
    "        user_date_convert = datetime.datetime.strptime(user_date, '%b %d, %Y')\n",
    "    except ValueError:\n",
    "        print(\"Incorrect Format\")\n",
    "    \n",
    "    blog_data = getData_date(user_date_convert)\n",
    "\n",
    "else:\n",
    "    print(\"Your choice is invalid, please re-enter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}